# Grafana Alert Rules Provisioning File
# File: llm-alert-rules.yaml
# Location: Place in Grafana provisioning/alerting directory
#
# These alert rules implement the LLM Passthrough Service alerting requirements:
# - Critical: High Error Rate (5xx > 15% for 15 minutes)
# - Warning: Response Time Degradation (P95 > 10 seconds for 15 minutes)
#
# Polling: Evaluated every 5 minutes (3 evaluations = 15 minutes)

apiVersion: 1

groups:
  - orgId: 1
    name: LLM Passthrough Service Alerts
    folder: LLM Observability
    interval: 5m  # Evaluation interval - polls every 5 minutes
    rules:
      # =============================================================================
      # CRITICAL ALERT: High Error Rate (5xx)
      # Condition: 5xx error rate > 15% for 15 minutes (3 consecutive evaluations)
      # =============================================================================
      - uid: llm-high-error-rate-critical
        title: "LLM Service - High Error Rate (5xx) - CRITICAL"
        condition: error_rate_threshold
        data:
          # Query A: Calculate 5xx error rate percentage
          - refId: A
            relativeTimeRange:
              from: 900  # 15 minutes
              to: 0
            datasourceUid: "${datasource}"
            model:
              editorMode: code
              expr: |
                (
                  sum(rate(http_request_count_total{namespace=~"ds-.*", name="service-large-language-model", http_status=~"5.."}[5m]))
                  /
                  sum(rate(http_request_count_total{namespace=~"ds-.*", name="service-large-language-model"}[5m]))
                ) * 100
              instant: true
              intervalMs: 1000
              legendFormat: "5xx Error Rate"
              maxDataPoints: 43200
              refId: A
          
          # Condition B: Threshold check - error rate > 15%
          - refId: error_rate_threshold
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 15  # Threshold: 15%
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              refId: error_rate_threshold
              type: threshold

        # Alert fires after 15 minutes (3 evaluations at 5-minute intervals)
        for: 15m
        
        annotations:
          summary: "LLM Passthrough Service experiencing high 5xx error rate"
          description: |
            The LLM Passthrough Service 5xx error rate has exceeded 15% for 15 minutes.
            
            Current error rate: {{ $values.A }}%
            Threshold: 15%
            
            This indicates server-side failures that require immediate investigation.
            
            Troubleshooting:
            1. Check LLM Service pods for errors: kubectl logs -n ds-* -l app=service-large-language-model
            2. Review upstream LLM provider status
            3. Check Arize for detailed error traces
            4. Review recent deployments
          runbook_url: "https://engconf.int.kronos.com/spaces/AI/pages/llm-runbook"
          dashboard_uid: llm-passthrough-unified-observability
          panel_id: "2"  # 5xx Error Rate panel
        
        labels:
          severity: critical
          service: llm-passthrough
          team: ai-engineering
          alert_type: error_rate
        
        noDataState: NoData
        execErrState: Error

      # =============================================================================
      # WARNING ALERT: Response Time Degradation
      # Condition: P95 latency > 10 seconds for 15 minutes (3 consecutive evaluations)
      # =============================================================================
      - uid: llm-response-time-degradation-warning
        title: "LLM Service - Response Time Degradation (P95) - WARNING"
        condition: latency_threshold
        data:
          # Query A: Calculate P95 latency in seconds
          - refId: A
            relativeTimeRange:
              from: 900  # 15 minutes
              to: 0
            datasourceUid: "${datasource}"
            model:
              editorMode: code
              expr: |
                histogram_quantile(0.95, 
                  sum by (le) (
                    rate(request_incoming_duration_seconds_bucket{namespace=~"ds-.*", name="service-large-language-model"}[5m])
                  )
                )
              instant: true
              intervalMs: 1000
              legendFormat: "P95 Latency"
              maxDataPoints: 43200
              refId: A
          
          # Condition B: Threshold check - P95 > 10 seconds
          - refId: latency_threshold
            relativeTimeRange:
              from: 900
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 10  # Threshold: 10 seconds
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              refId: latency_threshold
              type: threshold

        # Alert fires after 15 minutes (3 evaluations at 5-minute intervals)
        for: 15m
        
        annotations:
          summary: "LLM Passthrough Service experiencing high response latency"
          description: |
            The LLM Passthrough Service P95 latency has exceeded 10 seconds for 15 minutes.
            
            Current P95 latency: {{ $values.A }} seconds
            Threshold: 10 seconds
            
            This indicates performance degradation affecting user experience.
            
            Troubleshooting:
            1. Check LLM Service resource utilization (CPU/Memory)
            2. Review upstream LLM provider latency in Arize
            3. Check for request queue buildup
            4. Analyze traffic patterns for unusual load
            5. Review model inference times
          runbook_url: "https://engconf.int.kronos.com/spaces/AI/pages/llm-runbook"
          dashboard_uid: llm-passthrough-unified-observability
          panel_id: "12"  # P95 Latency panel
        
        labels:
          severity: warning
          service: llm-passthrough
          team: ai-engineering
          alert_type: latency
        
        noDataState: NoData
        execErrState: Error

# =============================================================================
# CONTACT POINTS CONFIGURATION
# Configure these in Grafana UI or via separate provisioning file
# =============================================================================
# 
# For PagerDuty integration:
# 1. Go to Grafana > Alerting > Contact Points
# 2. Add new contact point: "LLM-PagerDuty"
# 3. Select "PagerDuty" integration type
# 4. Enter Integration Key from PagerDuty
# 5. Set severity mapping:
#    - critical -> P1
#    - warning -> P3
#
# For Slack integration:
# 1. Go to Grafana > Alerting > Contact Points
# 2. Add new contact point: "LLM-Slack"
# 3. Select "Slack" integration type
# 4. Enter Webhook URL
# 5. Configure channel: #llm-alerts
#
# =============================================================================

# =============================================================================
# NOTIFICATION POLICIES
# Route alerts to appropriate contact points
# =============================================================================
# 
# Example notification policy (configure in Grafana UI):
# 
# Root Policy:
#   - Default contact point: LLM-Slack
#   
# Child Policies:
#   - Match: severity=critical
#     Contact point: LLM-PagerDuty
#     Continue: true (also send to Slack)
#   
#   - Match: severity=warning
#     Contact point: LLM-Slack
#     Mute timings: business-hours-only (optional)
#
# =============================================================================
